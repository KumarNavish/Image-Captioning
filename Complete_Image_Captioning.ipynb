{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk   ## NLP Library\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO  ## Library for using COCO Dataset\n",
    "import os\n",
    "from PIL import Image ## Image Processing\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Building Vocabulory from Captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.TreebankWordTokenizer().tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "def main():\n",
    "    caption_path='/home/navish/Desktop/MSCOCO/annotations/captions_train2014.json'\n",
    "    vocab_path='/home/navish/Desktop/MSCOCO/vocab.pkl'\n",
    "    threshold=4\n",
    "    vocab = build_vocab(json=caption_path, threshold=threshold)\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab, f)\n",
    "    print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "    print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabulary at 0x7fa0e8298550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/home/navish/Desktop/MSCOCO/vocab.pkl','rb') as fp:\n",
    "    vocab=pickle.load(fp)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing Images and Storing them in Parent Directory :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def resize_image(image, size):\n",
    "    \"\"\"Resize an image to the given size.\"\"\"\n",
    "    return image.resize(size, Image.ANTIALIAS)\n",
    "\n",
    "def resize_images(image_dir, output_dir, size):\n",
    "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    for i, image in enumerate(images):\n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = resize_image(img, size)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_images, output_dir))\n",
    "\n",
    "def main():\n",
    "    image_dir = '/home/navish/Desktop/MSCOCO/test2014/'\n",
    "    output_dir = '/home/navish/Desktop/MSCOCO/resizedval2014/'\n",
    "    image_size = (256,256)\n",
    "    resize_images(image_dir, output_dir, image_size)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading | Format required for Pytorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        img_id = coco.anns[ann_id]['image_id']\n",
    "#         print(coco.loadImgs(img_id)[0])\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "#         path = '/home/navish/Desktop/MSCOCO/output_dir/'\n",
    "#         try:\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "                # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.TreebankWordTokenizer().tokenize(caption.lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "\n",
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       json=json,\n",
    "                       vocab=vocab,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Encoder and Decoder Architechtures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.autograd.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model to reduce Perplexity : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# Device configuration\n",
    "x=torch.cuda.current_device()\n",
    "# device = torch.cuda.device('cuda')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.cuda.device(x)\n",
    "\n",
    "def main():\n",
    "     \n",
    "    model_path='/home/navish/Desktop/MSCOCO/models/'\n",
    "    crop_size=224 \n",
    "    vocab_path='/home/navish/Desktop/MSCOCO/vocab.pkl'\n",
    "    image_dir='/home/navish/Desktop/MSCOCO/resized2014'\n",
    "    caption_path='/home/navish/Desktop/MSCOCO/annotations/captions_train2014.json'\n",
    "    log_step=10\n",
    "    save_step=100\n",
    "    \n",
    "    # Model parameters\n",
    "    embed_size=256\n",
    "    hidden_size=512\n",
    "    num_layers=1\n",
    "    \n",
    "    num_epochs=60\n",
    "    batch_size=128\n",
    "    num_workers=2\n",
    "    learning_rate=0.001 #1e-10\n",
    "    # Create model directory\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    \n",
    "    # Image preprocessing, normalization for the pretrained resnet\n",
    "    transform = transforms.Compose([ \n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load vocabulary wrapper\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Build data loader\n",
    "    data_loader = get_loader(image_dir,caption_path, vocab, \n",
    "                             transform, batch_size,\n",
    "                             shuffle=True, num_workers=num_workers) \n",
    "    \n",
    "    # Build the models\n",
    "    encoder = EncoderCNN(embed_size).to(device=device,dtype=torch.float32,non_blocking=True)\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device=device,dtype=torch.float32,non_blocking=True)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "    \n",
    "    \n",
    "            \n",
    "   \n",
    "    # Train the models\n",
    "    total_step = len(data_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, captions, lengths) in enumerate(data_loader):            \n",
    "            # Set mini-batch dataset\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]        \n",
    "            # Forward, backward and optimize\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            outputs.requires_grad_(True)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            \n",
    "           \n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print log info\n",
    "            if i % log_step == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, num_epochs, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "                \n",
    "              \n",
    "            # Save the model checkpoints\n",
    "            if (i+1) % save_step == 0:\n",
    "                torch.save(decoder.state_dict(), os.path.join(\n",
    "                    model_path, 'decoder-bid-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
    "                torch.save(encoder.state_dict(), os.path.join(\n",
    "                    model_path, 'encoder-bid-{}-{}.ckpt'.format(epoch+1, i+1)))   \n",
    "                \n",
    "            \n",
    "              \n",
    "                \n",
    "if __name__ == '__main__':\n",
    "        main() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some of the images are in __grey scale__ so they need to be converted into __RGB Channels__ for formatted Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grey_scale to 3 channel conversion\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "\n",
    "files = [f for f in listdir('/home/navish/Desktop/MSCOCO/resizedval2014/') if isfile(join('/home/nsvish/Desktop/MSCOCO/resizedval2014/', f))]\n",
    "for i in range(0,len(files)):\n",
    "    img =Image.open('/home/navish/Desktop/MSCOCO/resizedval2014/'+files[i])\n",
    "    nchannels=3\n",
    "    A= np.asarray(img)\n",
    "    if ((A.shape==(256,256))):\n",
    "        print(i)\n",
    "        stacked_img = np.stack((A,)*3, -1)\n",
    "        nimg = Image.fromarray(stacked_img, 'RGB')\n",
    "        nimg.save('/home/navish/Desktop/MSCOCO/resizedval2014/'+files[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Real Caption Generation for Test-Images(Without Language Modelling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pickle \n",
    "import os\n",
    "from torchvision import transforms \n",
    "#from build_vocab import Vocabulary\n",
    "#from model import EncoderCNN, DecoderRNN3\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def main():\n",
    "    \n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    files = [f for f in listdir('/home/navish/Desktop/MSCOCO/resizedval2014/') if isfile(join('/home/navish/Desktop/MSCOCO/resizedval2014/', f))]\n",
    "    out_cap=[]\n",
    "    for i in range(0,len(files)):\n",
    "        image='/home/navish/Desktop/MSCOCO/resizedval2014/'+files[i]\n",
    "        encoder_path='/home/navish/Desktop/MSCOCO/models/encoder-bid-60-3200.ckpt'\n",
    "        decoder_path='/home/navish/Desktop/MSCOCO/models/decoder-bid-60-3200.ckpt'\n",
    "        vocab_path='/home/navish/Desktop/MSCOCO/vocab.pkl'\n",
    "\n",
    "        # Model parameters (should be same as paramters in train.py)\n",
    "        embed_size=256\n",
    "        hidden_size=512\n",
    "        num_layers=1\n",
    "\n",
    "        # Image preprocessing\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                 (0.229, 0.224, 0.225))])\n",
    "\n",
    "        # Load vocabulary wrapper\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            vocab = pickle.load(f)\n",
    "\n",
    "        # Build models\n",
    "        encoder = EncoderCNN(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "        decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n",
    "        encoder = encoder.to(device)\n",
    "        decoder = decoder.to(device)\n",
    "\n",
    "        # Load the trained model parameters\n",
    "        encoder.load_state_dict(torch.load(encoder_path))\n",
    "        decoder.load_state_dict(torch.load(decoder_path))\n",
    "\n",
    "        # Prepare an image\n",
    "        image = load_image(image, transform)\n",
    "        image_tensor = image.to(device)\n",
    "        #print(image_tensor[0])\n",
    "        # Generate an caption from the image\n",
    "        feature = encoder(image_tensor)\n",
    "        sampled_ids = decoder.sample(feature)\n",
    "        sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
    "\n",
    "        # Convert word_ids to words\n",
    "        sampled_caption = []\n",
    "        for word_id in sampled_ids:\n",
    "            word = vocab.idx2word[word_id]\n",
    "            sampled_caption.append(word)\n",
    "            if word == '<end>':\n",
    "                break\n",
    "        sentence = ' '.join(sampled_caption)\n",
    "        print(sentence)\n",
    "        # Print out the image and the generated caption\n",
    "        out_cap.append(sentence)\n",
    "        print (\"appended\", i)\n",
    "    out_caption_path='/home/navish/Desktop/MSCOCO/out_cap.pkl'    \n",
    "    with open(out_caption_path, 'wb') as g:\n",
    "        pickle.dump(out_cap, g)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Mapping ann_id to img_id__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "ann2img={}\n",
    "json='/home/sanjeet/Desktop/MSCOCO/annotations/captions_test2014.json'\n",
    "coco = COCO(json)\n",
    "ids = list(coco.anns.keys())\n",
    "for index in range(0,len(ids)):\n",
    "    ann_id = ids[index]\n",
    "    img_id = coco.anns[ann_id]['image_id']\n",
    "    ann2img.setdefault(img_id, []).append(ann_id)\n",
    "print(ann2img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Loss Scores :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "vocab_path='/home/navish/Desktop/MSCOCO/out_cap.pkl'\n",
    "with open(vocab_path, 'rb') as g:\n",
    "    out_caption = pickle.load(g)\n",
    "    \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "scores=[]\n",
    "files = [f for f in listdir('/home/navish/Desktop/MSCOCO/resizedval2014/') if isfile(join('/home/navish/Desktop/MSCOCO/resizedval2014/', f))]\n",
    "for i in range(0,len(files)):\n",
    "#     print(int(files[i][20:26]))\n",
    "    indices=ann2img[int(files[i][20:26])]\n",
    "    reference=[]\n",
    "    for ann_id in indices:\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        reference.append(caption)\n",
    "        print(\"Reference Caption \",ann_id,\" : \",caption)\n",
    "    candidate=out_caption[i][8:-6]  \n",
    "    #print(\"Machine Generste : \",candidate)\n",
    "    print(sentence_bleu(reference, candidate))\n",
    "    scores.append(sentence_bleu(reference, candidate))\n",
    "print(\"Average = \",(sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import json \n",
    "data = []  \n",
    "vocab_path='/home/navish/Desktop/MSCOCO/out_cap.pkl'\n",
    "with open(vocab_path, 'rb') as g:\n",
    "    out_caption = pickle.load(g)\n",
    "\n",
    "files = [f for f in listdir('/home/navish/Desktop/MSCOCO/resizedval2014/') if isfile(join('/home/nsvish/Desktop/MSCOCO/resizedval2014/', f))]\n",
    "for i in range(0,len(files)):\n",
    "    print(files[i][19:25])\n",
    "    print(int(files[i][19:25]))\n",
    "    candidate=out_caption[i][8:-6]   \n",
    "    #print(\"Machine Generste : \",candidate)\n",
    "    print( candidate)\n",
    "    data.append({  \n",
    "    \n",
    "    'image_id': int(files[i][19:25]),\n",
    "        'caption':candidate \n",
    "    })\n",
    "with open('apiData.json', 'w') as outfile:  \n",
    "    json.dump(data, outfile)  \n",
    "    \n",
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modelling to improve Caption quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import tempfile\n",
    "import itertools\n",
    "\n",
    "# path to the stanford corenlp jar\n",
    "STANFORD_CORENLP_3_4_1_JAR = '/home/navish/Downloads/stanford-corenlp-full-2018-02-27/stanford-corenlp-3.9.1.jar'\n",
    "\n",
    "# punctuations to be removed from the sentences\n",
    "PUNCTUATIONS = [\"''\", \"'\", \"``\", \"`\", \"-LRB-\", \"-RRB-\", \"-LCB-\", \"-RCB-\", \\\n",
    "        \".\", \"?\", \"!\", \",\", \":\", \"-\", \"--\", \"...\", \";\"] \n",
    "\n",
    "class PTBTokenizer:\n",
    "    \"\"\"Python wrapper of Stanford PTBTokenizer\"\"\"\n",
    "\n",
    "    def tokenize(self, captions_for_image):\n",
    "        cmd = ['java', '-cp', STANFORD_CORENLP_3_9_1_JAR, \\\n",
    "                'edu.stanford.nlp.process.PTBTokenizer', \\\n",
    "                '-preserveLines', '-lowerCase']\n",
    "\n",
    "        # ======================================================\n",
    "        # prepare data for PTB Tokenizer\n",
    "        # ======================================================\n",
    "        final_tokenized_captions_for_image = {}\n",
    "        image_id = [k for k, v in captions_for_image.items() for _ in range(len(v))]\n",
    "        print (\"22\")\n",
    "        apa=bytes('caption', 'utf-8')\n",
    "        #apa=apa.encode('utf-8')\n",
    "        sentences = '\\n'.join([c[apa].replace('\\n', ' ') for k, v in captions_for_image.items() for c in v])\n",
    "        #sentences=sentences.encode() \n",
    "        print (\"33\")\n",
    "        # ======================================================\n",
    "        # save sentences to temporary file\n",
    "        # ======================================================\n",
    "        print(\"11\")\n",
    "        path_to_jar_dirname=os.path.dirname(os.path.abspath(__file__))\n",
    "        tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=path_to_jar_dirname)\n",
    "        tmp_file.write(sentences)\n",
    "        tmp_file.close()\n",
    "\n",
    "        # ======================================================\n",
    "        # tokenize sentence\n",
    "        # ======================================================\n",
    "        cmd.append(os.path.basename(tmp_file.name))\n",
    "        p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n",
    "                stdout=subprocess.PIPE)\n",
    "        token_lines = p_tokenizer.communicate(input=sentences.rstrip())[0]\n",
    "        lines = token_lines.split('\\n')\n",
    "        # remove temp file\n",
    "        os.remove(tmp_file.name)\n",
    "\n",
    "        # ======================================================\n",
    "        # create dictionary for tokenized captions\n",
    "        # ======================================================\n",
    "        for k, line in zip(image_id, lines):\n",
    "            if not k in final_tokenized_captions_for_image:\n",
    "                final_tokenized_captions_for_image[k] = []\n",
    "            tokenized_caption = ' '.join([w for w in line.rstrip().split(' ') \\\n",
    "                    if w not in PUNCTUATIONS])\n",
    "            final_tokenized_captions_for_image[k].append(tokenized_caption)\n",
    "\n",
    "        return final_tokenized_captions_for_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "class COCOEvalCap:\n",
    "    def __init__(self,images,gts,res):\n",
    "        self.evalImgs = []\n",
    "        self.eval = {}\n",
    "        self.imgToEval = {}\n",
    "        self.params = {'image_id': images}\n",
    "        self.gts = gts\n",
    "        self.res = res\n",
    "\n",
    "    def evaluate(self):\n",
    "        imgIds = self.params['image_id']\n",
    "        gts = self.gts\n",
    "        res = self.res\n",
    "\n",
    "        # =================================================\n",
    "        # Set up scorers\n",
    "        # =================================================\n",
    "        print('tokenization...')\n",
    "        tokenizer = PTBTokenizer()\n",
    "        gts  = tokenizer.tokenize(gts)\n",
    "        res = tokenizer.tokenize(res)\n",
    "\n",
    "        # =================================================\n",
    "        # Set up scorers\n",
    "        # =================================================\n",
    "        print('setting up scorers...')\n",
    "        scorers = [\n",
    "            (Bleu(4), [\"Bleu_1\", \"Bleu_2\", \"Bleu_3\", \"Bleu_4\"]),\n",
    "            (Meteor(),\"METEOR\"),\n",
    "            (Rouge(), \"ROUGE_L\"),\n",
    "            (Cider(), \"CIDEr\")\n",
    "        ]\n",
    "\n",
    "        # =================================================\n",
    "        # Compute scores\n",
    "        # =================================================\n",
    "        eval = {}\n",
    "        for scorer, method in scorers:\n",
    "            print ('computing %s score...'%(scorer.method()))\n",
    "            score, scores = scorer.compute_score(gts, res)\n",
    "            if type(method) == list:\n",
    "                for sc, scs, m in zip(score, scores, method):\n",
    "                    self.setEval(sc, m)\n",
    "                    self.setImgToEvalImgs(scs, imgIds, m)\n",
    "                    print (\"%s: %0.3f\"%(m, sc))\n",
    "            else:\n",
    "                self.setEval(score, method)\n",
    "                self.setImgToEvalImgs(scores, imgIds, method)\n",
    "                print(\"%s: %0.3f\"%(method, score))\n",
    "        self.setEvalImgs()\n",
    "\n",
    "    def setEval(self, score, method):\n",
    "        self.eval[method] = score\n",
    "\n",
    "    def setImgToEvalImgs(self, scores, imgIds, method):\n",
    "        for imgId, score in zip(imgIds, scores):\n",
    "            if not imgId in self.imgToEval:\n",
    "                self.imgToEval[imgId] = {}\n",
    "                self.imgToEval[imgId][\"image_id\"] = imgId\n",
    "            self.imgToEval[imgId][method] = score\n",
    "\n",
    "    def setEvalImgs(self):\n",
    "        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]\n",
    "\n",
    "\n",
    "def calculate_metrics(rng,datasetGTS,datasetRES):\n",
    "    imgIds = rng\n",
    "    gts = {}\n",
    "    res = {}\n",
    "\n",
    "    imgToAnnsGTS = {ann['image_id']: [] for ann in datasetGTS['annotations']}\n",
    "    for ann in datasetGTS['annotations']:\n",
    "        imgToAnnsGTS[ann['image_id']] += [ann]\n",
    "\n",
    "    imgToAnnsRES = {ann['image_id']: [] for ann in datasetRES['annotations']}\n",
    "    for ann in datasetRES['annotations']:\n",
    "        imgToAnnsRES[ann['image_id']] += [ann]\n",
    "\n",
    "    for imgId in imgIds:\n",
    "        gts[imgId] = imgToAnnsGTS[imgId]\n",
    "        res[imgId] = imgToAnnsRES[imgId]\n",
    "\n",
    "    evalObj = COCOEvalCap(imgIds,gts,res)\n",
    "    evalObj.evaluate()\n",
    "    return evalObj.eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pycocotools.coco import COCO\n",
    "import collections\n",
    "\n",
    "with open('apiData.json') as f:\n",
    "    d= json.load(f)\n",
    "da={}\n",
    "for i in range (len(d)):\n",
    "    da.setdefault((d[i]['image_id']), []).append(d[i]['caption'])\n",
    "\n",
    "json='/home/sanjeet/Desktop/MSCOCO/annotations/captions_val2014.json'\n",
    "coco = COCO(json)\n",
    "ids = list(coco.anns.keys())\n",
    "imgcap={}\n",
    "img_arr=[]\n",
    "for index in range(0,len(ids)):\n",
    "    ann_id = ids[index]\n",
    "    img_id = coco.anns[ann_id]['image_id']\n",
    "    img_arr.append(img_id)\n",
    "    cap= coco.anns[ann_id]['caption']\n",
    "    #ann2img.setdefault(img_id, []).append(ann_id)\n",
    "    imgcap.setdefault(img_id, []).append(cap)\n",
    "\n",
    "od = collections.OrderedDict(sorted(da.items()))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    rng = img_arr\n",
    "\n",
    "    res1=[]\n",
    "    gts1=[]\n",
    "    for k, v in od.items(): \n",
    "        #print(k, v)\n",
    "        a={u'image_id': k, u'caption': v[0]}\n",
    "        b1={u'image_id': k, u'caption': imgcap[k][0]}\n",
    "        b2={u'image_id': k, u'caption': imgcap[k][1]}\n",
    "        b3={u'image_id': k, u'caption': imgcap[k][2]}\n",
    "        b4={u'image_id': k, u'caption': imgcap[k][3]}\n",
    "        b5={u'image_id': k, u'caption': imgcap[k][4]}\n",
    "        res1.append(a)\n",
    "        gts1.append(b1)\n",
    "        gts1.append(b2)\n",
    "        gts1.append(b3)\n",
    "        gts1.append(b4)\n",
    "        gts1.append(b5)\n",
    "\n",
    "    datasetRES = {'annotations':res1} \n",
    "    datasetGTS = {'annotations': gts1}\n",
    "    print (calculate_metrics(rng,datasetGTS,datasetRES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
